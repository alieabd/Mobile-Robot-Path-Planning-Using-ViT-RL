{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "id": "S_Dw8wNS6Dcy",
    "ExecuteTime": {
     "end_time": "2023-08-25T10:13:33.432024600Z",
     "start_time": "2023-08-25T10:13:33.398758700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G:\\project\\fifth_try\\data\n"
     ]
    }
   ],
   "source": [
    "cd G:\\project\\fifth_try\\data"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "POg8CxGb6I3J",
    "outputId": "effbfd4b-57c9-43cf-8da0-0276632c54e8"
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "cd drive/MyDrive/"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mPmvvYFM6d3Y",
    "outputId": "fde45072-d6f8-416c-f484-904e9a9b10f2"
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/content/drive/MyDrive\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install transformers\n",
    "!pip install GPUtil"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v7BW0thZ6NGA",
    "outputId": "f07f9cb6-57cd-4b29-cd35-98ae29613ddc"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.31.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.0)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: GPUtil in /usr/local/lib/python3.10/dist-packages (1.4.0)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import ViTFeatureExtractor, ViTModel, ViTConfig, AutoConfig\n",
    "\n",
    "from PIL import Image\n",
    "from torchsummary import summary\n",
    "import GPUtil\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "id": "F9HcTJ-76Dcz",
    "ExecuteTime": {
     "end_time": "2023-08-25T10:13:41.548001900Z",
     "start_time": "2023-08-25T10:13:35.641744600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# import training data"
   ],
   "metadata": {
    "collapsed": false,
    "id": "YGSeZZty6Dcz"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "image = np.load(\"W_Image_data_per_episode.npy\",mmap_mode='c')\n",
    "episode_length = np.load(\"W_Episode.npy\",mmap_mode='c')\n",
    "velocity = np.load(\"W_Velocity.npy\",mmap_mode='c')\n",
    "r_theta_data = np.load(\"W_R_T_data.npy\",mmap_mode='c')\n",
    "\n",
    "\n",
    "test_image = np.load(\"W_test_Image_data_per_episode.npy\",mmap_mode='c')\n",
    "test_episode_length = np.load(\"W_test_Episode.npy\",mmap_mode='c')\n",
    "test_velocity = np.load(\"W_test_Velocity.npy\",mmap_mode='c')\n",
    "test_r_theta_data = np.load(\"W_test_R_T_data.npy\",mmap_mode='c')"
   ],
   "metadata": {
    "id": "Jg-TvrnA6Dc0",
    "ExecuteTime": {
     "end_time": "2023-08-25T11:02:36.626256800Z",
     "start_time": "2023-08-25T11:02:36.445174100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# first 10 episode of each try"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[    0,   533],\n       [ 2182,  2758],\n       [ 4330,  4928],\n       [ 6534,  7193],\n       [ 8760,  9379],\n       [11049, 11716],\n       [13307, 13834],\n       [15443, 15991],\n       [17772, 18377],\n       [19953, 20591]])"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desiger_ranges = np.array([[0,episode_length[9]],\n",
    "       [episode_length[49],episode_length[59]],\n",
    "       [episode_length[99],episode_length[109]],\n",
    "       [episode_length[149],episode_length[159]],\n",
    "       [episode_length[199],episode_length[209]],\n",
    "       [episode_length[249],episode_length[259]],\n",
    "       [episode_length[299],episode_length[309]],\n",
    "       [episode_length[349],episode_length[359]],\n",
    "       [episode_length[399],episode_length[409]],\n",
    "       [episode_length[449],episode_length[459]],\n",
    "       ]).astype(int)\n",
    "desiger_ranges"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-25T11:02:37.741867600Z",
     "start_time": "2023-08-25T11:02:37.717877300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "slices = np.r_[slice(0,533),slice(2182,2758),slice(4330,4928),slice(6534,7193),slice(8760,9379),\n",
    "                slice(11049,11716),slice(13307,13834),slice(15443,15991),slice(17772,18377),slice(19953,20591)]\n",
    "episode_slices = np.r_[slice(0,9),slice(49,59),slice(99,109),slice(149,159),slice(199,209),\n",
    "                        slice(249,259),slice(299,309),slice(349,359),slice(399,409),slice(449,459)]\n",
    "\n",
    "image = image[slices]\n",
    "episode_length = episode_length[episode_slices]\n",
    "velocity = velocity[slices]\n",
    "r_theta_data = r_theta_data[slices]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-25T11:02:40.098237Z",
     "start_time": "2023-08-25T11:02:38.484416300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "#image = image.astype(np.int32)\n",
    "#test_image = test_image.astype(np.int32)"
   ],
   "metadata": {
    "id": "agM-4Ynv6Dc0"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "image = np.reshape(image,[22218,120,160,3])\n",
    "test_image = np.reshape(test_image,[2266,120,160,3])"
   ],
   "metadata": {
    "id": "YtGzWQGe6Dc0"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "image_array = image[16855]"
   ],
   "metadata": {
    "id": "lExsjYC-6Dc0"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(image_array)\n",
    "plt.axis('off')  # Optional: turn off axis ticks and labels\n",
    "plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "idfoWGUu6Dc1",
    "outputId": "9a905649-c4f1-4514-b28b-707670be6597"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Rounding Velocity"
   ],
   "metadata": {
    "collapsed": false,
    "id": "hhmJlU3P6Dc1"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "#velocity = np.around(velocity,decimals=1)\n",
    "#r_theta_data = np.around(r_theta_data,decimals=2)\n",
    "\n",
    "\n",
    "#test_velocity = np.around(test_velocity,decimals=1)\n",
    "#test_r_theta_data = np.around(test_r_theta_data,decimals=2)"
   ],
   "metadata": {
    "id": "5sN-s9YB6Dc1"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# GPU setup"
   ],
   "metadata": {
    "collapsed": false,
    "id": "Ue4xOgb46Dc1"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Define function to add data/model in to GPU (cuda)\n",
    "def get_default_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "def to_device(data, device):\n",
    "    # if data is list or tuple, move each of them to device\n",
    "    if isinstance(data, (list, tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    def __init__(self, dl, device) -> None:\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "\n",
    "    def __iter__(self):\n",
    "        for b in self.dl:\n",
    "            # yield only execuate when the function is called\n",
    "            yield to_device(b, self. device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "\n",
    "device = get_default_device()"
   ],
   "metadata": {
    "id": "o2T8Rkeh6Dc2"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# prepare Input data"
   ],
   "metadata": {
    "collapsed": false,
    "id": "b3CkLlfo6Dc2"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "x_train shape: (22218, 120, 160, 3) - y_train shape: (22218, 2, 3)\n",
      "x_train shape: (2266, 120, 160, 3) - y_train shape: (2266, 2, 3)\n"
     ]
    }
   ],
   "source": [
    "input_shape = (120, 160, 3)\n",
    "num_epochs = 10\n",
    "\n",
    "x_train = image\n",
    "y_train = velocity\n",
    "\n",
    "x_test = test_image\n",
    "y_test = test_velocity\n",
    "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
    "print(f\"x_train shape: {x_test.shape} - y_train shape: {y_test.shape}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mWskvToQ6Dc2",
    "outputId": "b9300a31-ac90-46da-934a-328400d3a342"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "image = torch.tensor(image,device=device,dtype=torch.float32)\n",
    "r_theta_data = torch.tensor(r_theta_data,device=device,dtype=torch.float32)\n",
    "velocity = torch.tensor(velocity,device=device,dtype=torch.float32)"
   ],
   "metadata": {
    "id": "NSIGpzP1gvKe"
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# Define your MLP architecture (you can experiment with different architectures)\n",
    "class RelativeDistanceMLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(RelativeDistanceMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 512)  # Input size: input_dim, Output size: 128\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 1024)\n",
    "        self.fc4 = nn.Linear(1024, output_dim) # Output size adjusted to match the ViT feature extractor's output\n",
    "\n",
    "    def forward(self, relative_distance):\n",
    "        #print(f'relative distance $$$${relative_distance.shape}$$$$ relative distance')\n",
    "        x = torch.relu(self.fc1(relative_distance))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        #print(f'x    <<<<<<<<<{x.shape}>>>>>>>>>>   x')\n",
    "        return x"
   ],
   "metadata": {
    "id": "rS4TmJoe6Dc2"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define training dataset\n",
    "class robotDataset(Dataset):\n",
    "    def __init__(self, image, relative_distance, velocity, trans_transform=None):\n",
    "        self.vel = velocity\n",
    "        self.images = image\n",
    "        self.rel_dist = relative_distance\n",
    "        self.trans_transform = trans_transform\n",
    "        # Instantiate the MLP for encoding relative distance\n",
    "        output_dim = 1024\n",
    "        #self.mlp = RelativeDistanceMLP(input_dim=2, output_dim=output_dim)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vel)\n",
    "    def __getitem__(self, idx):\n",
    "        # Get image\n",
    "        image = self.images[idx]\n",
    "        # Preprocess image and extract patches using the ViT feature extractor\n",
    "        image_trans = self.trans_transform(image, return_tensors='pt')\n",
    "        #print(f'@@@@ {image_trans.shape}')\n",
    "        image_trans = image_trans['pixel_values'].squeeze()\n",
    "\n",
    "        # Get the corresponding relative distance\n",
    "        relative_distance = self.rel_dist[idx]\n",
    "\n",
    "        # Pass the relative distance through the MLP to obtain the encoded relative distance features\n",
    "        #encoded_relative_distance = self.mlp(relative_distance)\n",
    "        velocity = self.vel[idx]\n",
    "\n",
    "        #linear_velocity_x = torch.reshape(velocity[0, 0],[1])\n",
    "        #angular_velocity_z = torch.reshape(velocity[1, 2],[1])\n",
    "        #velocity = torch.cat((linear_velocity_x,angular_velocity_z))\n",
    "        velocity = torch.reshape(velocity[1, 2],[1])\n",
    "\n",
    "        return image_trans, relative_distance, velocity\n",
    "\n",
    "trans_transform = ViTFeatureExtractor.from_pretrained('google/vit-large-patch16-224')\n",
    "\n",
    "\n",
    "\n",
    "train_ds = robotDataset(image,r_theta_data,velocity, trans_transform=trans_transform)\n",
    "train_dl = DataLoader(train_ds, batch_size=1, shuffle=False)\n"
   ],
   "metadata": {
    "id": "Mgco6nay6Dc2",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d3e9acd5-d410-428f-cf93-d86080a240ca"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [],
   "metadata": {
    "id": "A5wJ5-je6Dc2"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Modify ViT"
   ],
   "metadata": {
    "collapsed": false,
    "id": "zbXik_FL6Dc2"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# Modify the model - ViT model\n",
    "model_trans = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "'''\n",
    "loads the complete pretrained ViTModel from HuggingFace Transformers. This includes all the model layers -\n",
    "the patch embedding, transformer encoder, layer normalization, classification head, etc.\n",
    "'''\n",
    "count = 0\n",
    "for child in model_trans.children():\n",
    "    count += 1\n",
    "    if count < 4:\n",
    "        #The first four layers are not trainable ==>  lower-level feature extraction\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "layers_trans = list(model_trans.children()) # Get all the layers from the Transformer model\n",
    "model_trans_top = nn.Sequential(*layers_trans[:-2]) # Remove the normalization layer and pooler layer\n",
    "trans_layer_norm = list(model_trans.children())[2] # Get the normalization layer\n",
    "\n",
    "MLP = RelativeDistanceMLP(2,2048)"
   ],
   "metadata": {
    "id": "SZZCblFp6Dc2"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# The Model"
   ],
   "metadata": {
    "collapsed": false,
    "id": "eDWVUz9a6Dc3"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "class model_final(nn.Module):\n",
    "    def __init__(self, model_trans_top, trans_layer_norm, MLP,dp_rate=0.2):\n",
    "        super().__init__()\n",
    "        # All the trans model layers\n",
    "        self.model_trans_top = model_trans_top\n",
    "        self.trans_layer_norm = trans_layer_norm\n",
    "        self.trans_flatten = nn.Flatten()\n",
    "        self.trans_linear = nn.Linear(150528, 2048)\n",
    "        self.dropout = nn.Dropout(dp_rate)\n",
    "\n",
    "        self.MLP = MLP\n",
    "\n",
    "        # Merge the result and pass the\n",
    "        self.dropout = nn.Dropout(dp_rate)\n",
    "        self.linear1 = nn.Linear(4096, 2048)\n",
    "        self.linear2 = nn.Linear(2048,512)\n",
    "        self.linear3 = nn.Linear(512,128)\n",
    "        self.linear4 = nn.Linear(128,8)\n",
    "        self.linear5 = nn.Linear(8,1)\n",
    "\n",
    "    def forward(self, trans_b, MLP_b):\n",
    "        #transe_b Shape ==>[32,3,224,224]\n",
    "        # Get intermediate outputs using hidden layer\n",
    "        result_trans = self.model_trans_top(trans_b)\n",
    "        patch_state = result_trans.last_hidden_state[:,1:,:] # Remove the classification token and get the last hidden state of all patchs\n",
    "        result_trans = self.trans_layer_norm(patch_state)\n",
    "        result_trans = self.trans_flatten(result_trans) #instead of \"result_trans\" it was patch_state\n",
    "        result_trans = self.dropout(result_trans)\n",
    "        result_trans = self.trans_linear(result_trans) #[batch_size, 2048]\n",
    "\n",
    "\n",
    "\n",
    "        resault_MLP = self.MLP(MLP_b)\n",
    "        resault_MLP = torch.reshape(resault_MLP,[1,2048])\n",
    "\n",
    "        #Merge The Resault\n",
    "\n",
    "        result_merge = torch.cat((result_trans, resault_MLP),0)\n",
    "        result_merge = torch.reshape(result_merge,[1,4096])\n",
    "        result_merge = self.dropout(result_merge)\n",
    "        result_merge = self.linear1(result_merge)\n",
    "        result_merge = self.dropout(result_merge)\n",
    "        result_merge = self.linear2(result_merge)\n",
    "        result_merge = self.linear3(result_merge)\n",
    "        result_merge = self.linear4(result_merge)\n",
    "        result_merge = self.linear5(result_merge)\n",
    "\n",
    "        return result_merge\n",
    "\n",
    "model = model_final(model_trans_top, trans_layer_norm, MLP)\n",
    "# model.load_state_dict(torch.load('model_weights_1228'))"
   ],
   "metadata": {
    "id": "aZfXFOcV6Dc3"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Add data and model to GPU"
   ],
   "metadata": {
    "collapsed": false,
    "id": "DqmnZeig6Dc3"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "train_dl = DeviceDataLoader(train_dl, device)\n",
    "model = to_device(model, device)\n"
   ],
   "metadata": {
    "id": "uuogDvsd6Dc3"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define Optimizer and LearningRate scheduler"
   ],
   "metadata": {
    "collapsed": false,
    "id": "Rji3vpnS6Dc3"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "params = [param for param in list(model.parameters()) if param.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=1e-7, momentum=0.2)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=0.1,\n",
    "    patience=2,\n",
    "    verbose=True)"
   ],
   "metadata": {
    "id": "ZlnIu_kj6Dc3"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fit function AND doing grad steps"
   ],
   "metadata": {
    "collapsed": false,
    "id": "TkEFjpMU6Dc3"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def fit(epochs, model, train_dl):\n",
    "    opt = optimizer\n",
    "    sched = lr_scheduler\n",
    "    loss_func = nn.MSELoss()\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        batch_num = 1\n",
    "        for x_trans, x_MLP, yb in train_dl:\n",
    "            # Pass the opt so that funciton will get trained\n",
    "            total_loss = 0\n",
    "            preds = model(x_trans,x_MLP)\n",
    "            loss = loss_func(preds.squeeze(), yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "            print('\\r', f'batch #{batch_num}: {loss}', end='')\n",
    "            batch_num += 1\n",
    "            total_loss += loss.item()\n",
    "        sched.step(total_loss)\n",
    "        print('\\n', f'Epoch: ({epoch+1}/{epochs}) Loss = {total_loss}')"
   ],
   "metadata": {
    "id": "-dSJ50ZO6Dc3"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " batch #22218: 0.013353666290640831\n",
      " Epoch: (1/10) Loss = 0.013353666290640831\n",
      " batch #22218: 0.013697720132768154\n",
      " Epoch: (2/10) Loss = 0.013697720132768154\n",
      " batch #22218: 0.011208043433725834\n",
      " Epoch: (3/10) Loss = 0.011208043433725834\n",
      " batch #22218: 0.01303979940712452\n",
      " Epoch: (4/10) Loss = 0.01303979940712452\n",
      " batch #22218: 0.010191072709858418\n",
      " Epoch: (5/10) Loss = 0.010191072709858418\n",
      " batch #22218: 0.007587056141346693\n",
      " Epoch: (6/10) Loss = 0.007587056141346693\n",
      " batch #22218: 0.009119482710957527\n",
      " Epoch: (7/10) Loss = 0.009119482710957527\n",
      " batch #22218: 0.0073139481246471405\n",
      " Epoch: (8/10) Loss = 0.0073139481246471405\n",
      " batch #12881: 8.961454113887157e-06"
     ]
    }
   ],
   "source": [
    "# Training the model and save weights\n",
    "fit(num_epochs, model, train_dl)\n",
    "torch.save(model.state_dict(), \"model_weights\")"
   ],
   "metadata": {
    "id": "9jUz2KGx6Dc3",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "39abfa4d-1f7c-4e29-e65d-3ecb0f49c3ac"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "id": "zTWJEYXA6Dc3"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "id": "HFYdsRgN6Dc3"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "id": "KVkulQ_l6Dc3"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "id": "rfVEuNp86Dc4"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "id": "5yAcaRyX6Dc4"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "id": "Xdzx25Oa6Dc4"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "id": "RmJpuZZM6Dc4"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "id": "53bIarB76Dc4"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "id": "fcg7wmCf6Dc4"
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
