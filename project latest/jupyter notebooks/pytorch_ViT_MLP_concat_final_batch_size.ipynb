{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "id": "S_Dw8wNS6Dcy",
    "ExecuteTime": {
     "end_time": "2023-08-20T22:29:35.026129300Z",
     "start_time": "2023-08-20T22:29:34.987128700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G:\\project\\fifth_try\\data\n"
     ]
    }
   ],
   "source": [
    "cd G:\\project\\fifth_try\\data"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "POg8CxGb6I3J",
    "outputId": "effbfd4b-57c9-43cf-8da0-0276632c54e8"
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "cd drive/MyDrive/"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mPmvvYFM6d3Y",
    "outputId": "fde45072-d6f8-416c-f484-904e9a9b10f2"
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/content/drive/MyDrive\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install transformers\n",
    "!pip install GPUtil"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v7BW0thZ6NGA",
    "outputId": "f07f9cb6-57cd-4b29-cd35-98ae29613ddc"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.31.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.0)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: GPUtil in /usr/local/lib/python3.10/dist-packages (1.4.0)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import ViTFeatureExtractor, ViTModel, ViTConfig, AutoConfig\n",
    "\n",
    "from PIL import Image\n",
    "from torchsummary import summary\n",
    "import GPUtil\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "id": "F9HcTJ-76Dcz",
    "ExecuteTime": {
     "end_time": "2023-08-20T22:29:45.988906200Z",
     "start_time": "2023-08-20T22:29:39.362432300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# import training data"
   ],
   "metadata": {
    "collapsed": false,
    "id": "YGSeZZty6Dcz"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "image = np.load(\"W_Image_data_per_episode.npy\",mmap_mode='c')\n",
    "episode_length = np.load(\"W_Episode.npy\",mmap_mode='c')\n",
    "velocity = np.load(\"W_Velocity.npy\",mmap_mode='c')\n",
    "r_theta_data = np.load(\"W_R_T_data.npy\",mmap_mode='c')\n",
    "\n",
    "\n",
    "test_image = np.load(\"W_test_Image_data_per_episode.npy\",mmap_mode='c')\n",
    "test_episode_length = np.load(\"W_test_Episode.npy\",mmap_mode='c')\n",
    "test_velocity = np.load(\"W_test_Velocity.npy\",mmap_mode='c')\n",
    "test_r_theta_data = np.load(\"W_test_R_T_data.npy\",mmap_mode='c')"
   ],
   "metadata": {
    "id": "Jg-TvrnA6Dc0",
    "ExecuteTime": {
     "end_time": "2023-08-20T22:29:48.089278300Z",
     "start_time": "2023-08-20T22:29:48.015728100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "#image = image.astype(np.int32)\n",
    "#test_image = test_image.astype(np.int32)"
   ],
   "metadata": {
    "id": "agM-4Ynv6Dc0",
    "ExecuteTime": {
     "end_time": "2023-08-20T22:29:49.053222600Z",
     "start_time": "2023-08-20T22:29:49.030220200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "image = np.reshape(image,[22218,120,160,3])\n",
    "test_image = np.reshape(test_image,[2266,120,160,3])"
   ],
   "metadata": {
    "id": "YtGzWQGe6Dc0",
    "ExecuteTime": {
     "end_time": "2023-08-20T22:29:49.562786100Z",
     "start_time": "2023-08-20T22:29:49.546788100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "image_array = image[16855]"
   ],
   "metadata": {
    "id": "lExsjYC-6Dc0",
    "ExecuteTime": {
     "end_time": "2023-08-20T22:29:49.927934800Z",
     "start_time": "2023-08-20T22:29:49.903936700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATAAAADnCAYAAACZtwrQAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAADH0lEQVR4nO3ZsQnAMAwAwTh4/5WdCYLBRcLDXatG1SPQWGtdAEX33wsAnBIwIEvAgCwBA7IEDMiam7kXJfC38TZwgQFZAgZkCRiQJWBAloABWQIGZAkYkCVgQJaAAVkCBmQJGJAlYECWgAFZAgZkCRiQJWBAloABWQIGZAkYkCVgQJaAAVkCBmQJGJAlYECWgAFZAgZkCRiQJWBAloABWQIGZAkYkCVgQJaAAVkCBmQJGJAlYECWgAFZAgZkCRiQJWBAloABWQIGZAkYkCVgQJaAAVkCBmQJGJAlYECWgAFZAgZkCRiQJWBAloABWQIGZAkYkCVgQJaAAVkCBmQJGJAlYECWgAFZAgZkCRiQJWBAloABWQIGZAkYkCVgQJaAAVkCBmQJGJAlYECWgAFZAgZkCRiQJWBAloABWQIGZAkYkCVgQJaAAVkCBmQJGJAlYECWgAFZAgZkCRiQJWBAloABWQIGZAkYkCVgQJaAAVkCBmQJGJAlYECWgAFZAgZkCRiQJWBAloABWQIGZAkYkCVgQJaAAVkCBmQJGJAlYECWgAFZAgZkCRiQJWBAloABWQIGZAkYkCVgQJaAAVkCBmQJGJAlYECWgAFZAgZkCRiQJWBAloABWQIGZAkYkCVgQJaAAVkCBmQJGJAlYECWgAFZAgZkCRiQJWBAloABWQIGZAkYkCVgQJaAAVkCBmQJGJAlYECWgAFZAgZkCRiQJWBAloABWQIGZAkYkCVgQJaAAVkCBmQJGJAlYECWgAFZAgZkCRiQJWBAloABWQIGZAkYkCVgQJaAAVkCBmQJGJAlYECWgAFZAgZkCRiQJWBAloABWQIGZAkYkCVgQJaAAVkCBmQJGJAlYECWgAFZAgZkCRiQJWBAloABWQIGZAkYkCVgQJaAAVkCBmQJGJAlYECWgAFZAgZkCRiQJWBAloABWQIGZAkYkCVgQJaAAVkCBmQJGJAlYECWgAFZAgZkCRiQJWBAloABWQIGZAkYkCVgQJaAAVkCBmQJGJAlYECWgAFZczMfn2wBcMAFBmQJGJAlYECWgAFZAgZkCRiQ9QA5LAXQiW8L8wAAAABJRU5ErkJggg==\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(image_array)\n",
    "plt.axis('off')  # Optional: turn off axis ticks and labels\n",
    "plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "idfoWGUu6Dc1",
    "outputId": "9a905649-c4f1-4514-b28b-707670be6597",
    "ExecuteTime": {
     "end_time": "2023-08-20T22:29:50.293760Z",
     "start_time": "2023-08-20T22:29:50.180244300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Rounding Velocity"
   ],
   "metadata": {
    "collapsed": false,
    "id": "hhmJlU3P6Dc1"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "#velocity = np.around(velocity,decimals=1)\n",
    "#r_theta_data = np.around(r_theta_data,decimals=2)\n",
    "\n",
    "\n",
    "#test_velocity = np.around(test_velocity,decimals=1)\n",
    "#test_r_theta_data = np.around(test_r_theta_data,decimals=2)"
   ],
   "metadata": {
    "id": "5sN-s9YB6Dc1",
    "ExecuteTime": {
     "end_time": "2023-08-20T22:29:50.683883800Z",
     "start_time": "2023-08-20T22:29:50.654350400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# GPU setup"
   ],
   "metadata": {
    "collapsed": false,
    "id": "Ue4xOgb46Dc1"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Define function to add data/model in to GPU (cuda)\n",
    "def get_default_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "def to_device(data, device):\n",
    "    # if data is list or tuple, move each of them to device\n",
    "    if isinstance(data, (list, tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    def __init__(self, dl, device) -> None:\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "\n",
    "    def __iter__(self):\n",
    "        for b in self.dl:\n",
    "            # yield only execuate when the function is called\n",
    "            yield to_device(b, self. device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "\n",
    "device = get_default_device()"
   ],
   "metadata": {
    "id": "o2T8Rkeh6Dc2",
    "ExecuteTime": {
     "end_time": "2023-08-20T22:29:52.584078200Z",
     "start_time": "2023-08-20T22:29:52.542565700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# prepare Input data"
   ],
   "metadata": {
    "collapsed": false,
    "id": "b3CkLlfo6Dc2"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (22218, 120, 160, 3) - y_train shape: (22218, 2, 3)\n",
      "x_train shape: (2266, 120, 160, 3) - y_train shape: (2266, 2, 3)\n"
     ]
    }
   ],
   "source": [
    "input_shape = (120, 160, 3)\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "x_train = image\n",
    "y_train = velocity\n",
    "\n",
    "x_test = test_image\n",
    "y_test = test_velocity\n",
    "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
    "print(f\"x_train shape: {x_test.shape} - y_train shape: {y_test.shape}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mWskvToQ6Dc2",
    "outputId": "b9300a31-ac90-46da-934a-328400d3a342",
    "ExecuteTime": {
     "end_time": "2023-08-20T22:29:54.612444100Z",
     "start_time": "2023-08-20T22:29:54.596441800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "image = torch.tensor(image,device=device,dtype=torch.float32)\n",
    "r_theta_data = torch.tensor(r_theta_data,device=device,dtype=torch.float32)\n",
    "velocity = torch.tensor(velocity,device=device,dtype=torch.float32)"
   ],
   "metadata": {
    "id": "NSIGpzP1gvKe",
    "ExecuteTime": {
     "end_time": "2023-08-20T22:30:13.546134800Z",
     "start_time": "2023-08-20T22:29:56.001178100Z"
    }
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Define your MLP architecture (you can experiment with different architectures)\n",
    "class RelativeDistanceMLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(RelativeDistanceMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 512)  # Input size: input_dim, Output size: 128\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 1024)\n",
    "        self.fc4 = nn.Linear(1024, output_dim) # Output size adjusted to match the ViT feature extractor's output\n",
    "\n",
    "    def forward(self, relative_distance):\n",
    "        #print(f'relative distance $$$${relative_distance.shape}$$$$ relative distance')\n",
    "        x = torch.relu(self.fc1(relative_distance))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        #print(f'x    <<<<<<<<<{x.shape}>>>>>>>>>>   x')\n",
    "        return x"
   ],
   "metadata": {
    "id": "rS4TmJoe6Dc2",
    "ExecuteTime": {
     "end_time": "2023-08-20T22:30:15.165724700Z",
     "start_time": "2023-08-20T22:30:15.134737200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\imali\\miniconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\models\\vit\\feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define training dataset\n",
    "class robotDataset(Dataset):\n",
    "    def __init__(self, image, relative_distance, velocity, trans_transform=None):\n",
    "        self.vel = velocity\n",
    "        self.images = image\n",
    "        self.rel_dist = relative_distance\n",
    "        self.trans_transform = trans_transform\n",
    "        # Instantiate the MLP for encoding relative distance\n",
    "        output_dim = 1024\n",
    "        #self.mlp = RelativeDistanceMLP(input_dim=2, output_dim=output_dim)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vel)\n",
    "    def __getitem__(self, idx):\n",
    "        # Get image\n",
    "        image = self.images[idx]\n",
    "        # Preprocess image and extract patches using the ViT feature extractor\n",
    "        image_trans = [self.trans_transform(im, return_tensors='pt')['pixel_values'].squeeze() for im in image]\n",
    "        #print(f'@@@@ {image_trans.shape}')\n",
    "        image_trans = torch.stack(image_trans)\n",
    "\n",
    "        # Get the corresponding relative distance\n",
    "        relative_distance = self.rel_dist[idx]\n",
    "        relative_distance = torch.tensor(relative_distance, dtype=torch.float32)\n",
    "\n",
    "        # Pass the relative distance through the MLP to obtain the encoded relative distance features\n",
    "        #encoded_relative_distance = self.mlp(relative_distance)\n",
    "        velocity = self.vel[idx]\n",
    "\n",
    "        #linear_velocity_x = torch.reshape(velocity[0, 0],[1])\n",
    "        #angular_velocity_z = torch.reshape(velocity[1, 2],[1])\n",
    "        #velocity = torch.cat((linear_velocity_x,angular_velocity_z))\n",
    "        velocity = torch.tensor(velocity[:, 1, 2], dtype=torch.float32)  # Extract angular_velocity_z\n",
    "\n",
    "        return image_trans, relative_distance, velocity\n",
    "\n",
    "trans_transform = ViTFeatureExtractor.from_pretrained('google/vit-large-patch16-224')\n",
    "\n",
    "\n",
    "\n",
    "train_ds = robotDataset(image,r_theta_data,velocity, trans_transform=trans_transform)\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=False)\n"
   ],
   "metadata": {
    "id": "Mgco6nay6Dc2",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d3e9acd5-d410-428f-cf93-d86080a240ca",
    "ExecuteTime": {
     "end_time": "2023-08-20T22:30:17.327475900Z",
     "start_time": "2023-08-20T22:30:16.120782700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [],
   "metadata": {
    "id": "A5wJ5-je6Dc2",
    "ExecuteTime": {
     "end_time": "2023-08-20T22:30:17.367477800Z",
     "start_time": "2023-08-20T22:30:17.328480100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Modify ViT"
   ],
   "metadata": {
    "collapsed": false,
    "id": "zbXik_FL6Dc2"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# Modify the model - ViT model\n",
    "model_trans = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "'''\n",
    "loads the complete pretrained ViTModel from HuggingFace Transformers. This includes all the model layers -\n",
    "the patch embedding, transformer encoder, layer normalization, classification head, etc.\n",
    "'''\n",
    "count = 0\n",
    "for child in model_trans.children():\n",
    "    count += 1\n",
    "    if count < 4:\n",
    "        #The first four layers are not trainable ==>  lower-level feature extraction\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "layers_trans = list(model_trans.children()) # Get all the layers from the Transformer model\n",
    "model_trans_top = nn.Sequential(*layers_trans[:-2]) # Remove the normalization layer and pooler layer\n",
    "trans_layer_norm = list(model_trans.children())[2] # Get the normalization layer\n",
    "\n",
    "MLP = RelativeDistanceMLP(2,2048)"
   ],
   "metadata": {
    "id": "SZZCblFp6Dc2",
    "ExecuteTime": {
     "end_time": "2023-08-20T22:30:19.395665Z",
     "start_time": "2023-08-20T22:30:18.019042700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# The Model"
   ],
   "metadata": {
    "collapsed": false,
    "id": "eDWVUz9a6Dc3"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "class model_final(nn.Module):\n",
    "    def __init__(self, model_trans_top, trans_layer_norm, MLP,dp_rate=0.2):\n",
    "        super().__init__()\n",
    "        # All the trans model layers\n",
    "        self.model_trans_top = model_trans_top\n",
    "        self.trans_layer_norm = trans_layer_norm\n",
    "        self.trans_flatten = nn.Flatten()\n",
    "        self.trans_linear = nn.Linear(150528, 2048)\n",
    "        self.dropout = nn.Dropout(dp_rate)\n",
    "\n",
    "        self.MLP = MLP\n",
    "\n",
    "        # Merge the result and pass the\n",
    "        self.dropout = nn.Dropout(dp_rate)\n",
    "        self.linear1 = nn.Linear(4096, 2048)\n",
    "        self.linear2 = nn.Linear(2048,512)\n",
    "        self.linear3 = nn.Linear(512,128)\n",
    "        self.linear4 = nn.Linear(128,8)\n",
    "        self.linear5 = nn.Linear(8,1)\n",
    "\n",
    "    def forward(self, trans_b, MLP_b):\n",
    "        #transe_b Shape ==>[32,3,224,224]\n",
    "        # Get intermediate outputs using hidden layer\n",
    "        result_trans = self.model_trans_top(trans_b)\n",
    "        patch_state = result_trans.last_hidden_state[:,1:,:] # Remove the classification token and get the last hidden state of all patchs\n",
    "        result_trans = self.trans_layer_norm(patch_state)\n",
    "        result_trans = self.trans_flatten(result_trans) #instead of \"result_trans\" it was patch_state\n",
    "        result_trans = self.dropout(result_trans)\n",
    "        result_trans = self.trans_linear(result_trans) #[batch_size, 2048]\n",
    "\n",
    "\n",
    "\n",
    "        resault_MLP = self.MLP(MLP_b)\n",
    "        resault_MLP = torch.reshape(resault_MLP,[1,2048])\n",
    "\n",
    "        #Merge The Resault\n",
    "\n",
    "        result_merge = torch.cat((result_trans, resault_MLP),0)\n",
    "        result_merge = torch.reshape(result_merge,[1,4096])\n",
    "        result_merge = self.dropout(result_merge)\n",
    "        result_merge = self.linear1(result_merge)\n",
    "        result_merge = self.dropout(result_merge)\n",
    "        result_merge = self.linear2(result_merge)\n",
    "        result_merge = self.linear3(result_merge)\n",
    "        result_merge = self.linear4(result_merge)\n",
    "        result_merge = self.linear5(result_merge)\n",
    "\n",
    "        return result_merge\n",
    "\n",
    "model = model_final(model_trans_top, trans_layer_norm, MLP)\n",
    "# model.load_state_dict(torch.load('model_weights_1228'))"
   ],
   "metadata": {
    "id": "aZfXFOcV6Dc3",
    "ExecuteTime": {
     "end_time": "2023-08-20T22:30:20.847001300Z",
     "start_time": "2023-08-20T22:30:19.415170100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Add data and model to GPU"
   ],
   "metadata": {
    "collapsed": false,
    "id": "DqmnZeig6Dc3"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "train_dl = DeviceDataLoader(train_dl, device)\n",
    "model = to_device(model, device)\n"
   ],
   "metadata": {
    "id": "uuogDvsd6Dc3",
    "ExecuteTime": {
     "end_time": "2023-08-20T22:30:20.889021100Z",
     "start_time": "2023-08-20T22:30:20.858001Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define Optimizer and LearningRate scheduler"
   ],
   "metadata": {
    "collapsed": false,
    "id": "Rji3vpnS6Dc3"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "params = [param for param in list(model.parameters()) if param.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=1e-7, momentum=0.2)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=0.1,\n",
    "    patience=2,\n",
    "    verbose=True)"
   ],
   "metadata": {
    "id": "ZlnIu_kj6Dc3",
    "ExecuteTime": {
     "end_time": "2023-08-20T22:30:21.349483800Z",
     "start_time": "2023-08-20T22:30:21.332483200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fit function AND doing grad steps"
   ],
   "metadata": {
    "collapsed": false,
    "id": "TkEFjpMU6Dc3"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def fit(epochs, model, train_dl):\n",
    "    opt = optimizer\n",
    "    sched = lr_scheduler\n",
    "    loss_func = nn.MSELoss()\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        batch_num = 1\n",
    "        for x_trans, x_MLP, yb in train_dl:\n",
    "            # Pass the opt so that funciton will get trained\n",
    "            total_loss = 0\n",
    "            preds = model(x_trans,x_MLP)\n",
    "            yb = yb.reshape(batch_size,-1)\n",
    "            loss = loss_func(preds.squeeze(), yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "            print('\\r', f'batch #{batch_num}: {loss}', end='')\n",
    "            batch_num += 1\n",
    "            total_loss += loss.item()\n",
    "        sched.step(total_loss)\n",
    "        print('\\n', f'Epoch: ({epoch+1}/{epochs}) Loss = {total_loss}')"
   ],
   "metadata": {
    "id": "-dSJ50ZO6Dc3",
    "ExecuteTime": {
     "end_time": "2023-08-20T22:30:22.685237400Z",
     "start_time": "2023-08-20T22:30:22.659239100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid image shape. Expected either 4 or 3 dimensions, but got 2 dimensions.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [19]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Training the model and save weights\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dl\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m torch\u001B[38;5;241m.\u001B[39msave(model\u001B[38;5;241m.\u001B[39mstate_dict(), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel_weights\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Input \u001B[1;32mIn [18]\u001B[0m, in \u001B[0;36mfit\u001B[1;34m(epochs, model, train_dl)\u001B[0m\n\u001B[0;32m      6\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[0;32m      7\u001B[0m batch_num \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m----> 8\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m x_trans, x_MLP, yb \u001B[38;5;129;01min\u001B[39;00m train_dl:\n\u001B[0;32m      9\u001B[0m     \u001B[38;5;66;03m# Pass the opt so that funciton will get trained\u001B[39;00m\n\u001B[0;32m     10\u001B[0m     total_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m     11\u001B[0m     preds \u001B[38;5;241m=\u001B[39m model(x_trans,x_MLP)\n",
      "Input \u001B[1;32mIn [9]\u001B[0m, in \u001B[0;36mDeviceDataLoader.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__iter__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m---> 19\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m b \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdl:\n\u001B[0;32m     20\u001B[0m         \u001B[38;5;66;03m# yield only execuate when the function is called\u001B[39;00m\n\u001B[0;32m     21\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m to_device(b, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m device)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    630\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    631\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    632\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 633\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    634\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    635\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    636\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    637\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    675\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    676\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 677\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    678\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    679\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "Input \u001B[1;32mIn [13]\u001B[0m, in \u001B[0;36mrobotDataset.__getitem__\u001B[1;34m(self, idx)\u001B[0m\n\u001B[0;32m     16\u001B[0m image \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mimages[idx]\n\u001B[0;32m     17\u001B[0m \u001B[38;5;66;03m# Preprocess image and extract patches using the ViT feature extractor\u001B[39;00m\n\u001B[1;32m---> 18\u001B[0m image_trans \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrans_transform(im, return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m'\u001B[39m)[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpixel_values\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39msqueeze() \u001B[38;5;28;01mfor\u001B[39;00m im \u001B[38;5;129;01min\u001B[39;00m image]\n\u001B[0;32m     19\u001B[0m \u001B[38;5;66;03m#print(f'@@@@ {image_trans.shape}')\u001B[39;00m\n\u001B[0;32m     20\u001B[0m image_trans \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mstack(image_trans)\n",
      "Input \u001B[1;32mIn [13]\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     16\u001B[0m image \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mimages[idx]\n\u001B[0;32m     17\u001B[0m \u001B[38;5;66;03m# Preprocess image and extract patches using the ViT feature extractor\u001B[39;00m\n\u001B[1;32m---> 18\u001B[0m image_trans \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrans_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_tensors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mpt\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpixel_values\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39msqueeze() \u001B[38;5;28;01mfor\u001B[39;00m im \u001B[38;5;129;01min\u001B[39;00m image]\n\u001B[0;32m     19\u001B[0m \u001B[38;5;66;03m#print(f'@@@@ {image_trans.shape}')\u001B[39;00m\n\u001B[0;32m     20\u001B[0m image_trans \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mstack(image_trans)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\image_processing_utils.py:494\u001B[0m, in \u001B[0;36mBaseImageProcessor.__call__\u001B[1;34m(self, images, **kwargs)\u001B[0m\n\u001B[0;32m    492\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, images, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m BatchFeature:\n\u001B[0;32m    493\u001B[0m     \u001B[38;5;124;03m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 494\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpreprocess(images, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\models\\vit\\image_processing_vit.py:244\u001B[0m, in \u001B[0;36mViTImageProcessor.preprocess\u001B[1;34m(self, images, do_resize, size, resample, do_rescale, rescale_factor, do_normalize, image_mean, image_std, return_tensors, data_format, **kwargs)\u001B[0m\n\u001B[0;32m    241\u001B[0m size \u001B[38;5;241m=\u001B[39m size \u001B[38;5;28;01mif\u001B[39;00m size \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msize\n\u001B[0;32m    242\u001B[0m size_dict \u001B[38;5;241m=\u001B[39m get_size_dict(size)\n\u001B[1;32m--> 244\u001B[0m images \u001B[38;5;241m=\u001B[39m \u001B[43mmake_list_of_images\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimages\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    246\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m valid_images(images):\n\u001B[0;32m    247\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    248\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid image type. Must be of type PIL.Image.Image, numpy.ndarray, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    249\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtorch.Tensor, tf.Tensor or jax.ndarray.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    250\u001B[0m     )\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\image_utils.py:127\u001B[0m, in \u001B[0;36mmake_list_of_images\u001B[1;34m(images, expected_ndims)\u001B[0m\n\u001B[0;32m    125\u001B[0m         images \u001B[38;5;241m=\u001B[39m [images]\n\u001B[0;32m    126\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 127\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    128\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid image shape. Expected either \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mexpected_ndims \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m or \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mexpected_ndims\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m dimensions, but got\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    129\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mimages\u001B[38;5;241m.\u001B[39mndim\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m dimensions.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    130\u001B[0m         )\n\u001B[0;32m    131\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m images\n\u001B[0;32m    132\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    133\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid image type. Expected either PIL.Image.Image, numpy.ndarray, torch.Tensor, tf.Tensor or \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    134\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjax.ndarray, but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(images)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    135\u001B[0m )\n",
      "\u001B[1;31mValueError\u001B[0m: Invalid image shape. Expected either 4 or 3 dimensions, but got 2 dimensions."
     ]
    }
   ],
   "source": [
    "# Training the model and save weights\n",
    "fit(num_epochs, model, train_dl)\n",
    "torch.save(model.state_dict(), \"model_weights\")"
   ],
   "metadata": {
    "id": "9jUz2KGx6Dc3",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "39abfa4d-1f7c-4e29-e65d-3ecb0f49c3ac",
    "ExecuteTime": {
     "end_time": "2023-08-20T22:30:25.532948500Z",
     "start_time": "2023-08-20T22:30:23.962591700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "id": "zTWJEYXA6Dc3"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "id": "HFYdsRgN6Dc3"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "id": "KVkulQ_l6Dc3"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "id": "rfVEuNp86Dc4"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "id": "5yAcaRyX6Dc4"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "id": "Xdzx25Oa6Dc4"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "id": "RmJpuZZM6Dc4"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "id": "53bIarB76Dc4"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "id": "fcg7wmCf6Dc4"
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
