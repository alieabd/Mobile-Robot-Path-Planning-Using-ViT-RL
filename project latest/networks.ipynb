{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee17acfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-25 15:00:39.315831: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-04-25 15:00:39.348650: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-04-25 15:00:39.496850: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-04-25 15:00:39.497307: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-25 15:00:40.165903: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dropout,Dense,Flatten,Input,Activation,concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af088e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "M=5\n",
    "T=15\n",
    "#sig=0.5\n",
    "batch_size=10\n",
    "gamma=0.01\n",
    "alpha=0.3\n",
    "#a_1 = 0.5\n",
    "#a_2 = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "133e2546",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (160, 120, 3)\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "batch_size = 256\n",
    "num_epochs = 50\n",
    "image_size = 256  # We'll resize input images to this size\n",
    "patch_size = 32  # Size of the patches to be extract from the input images\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "projection_dim = 64\n",
    "num_heads = 4\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]  # Size of the transformer layers\n",
    "transformer_layers = 2\n",
    "mlp_head_units = [2048, 1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39cc83a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-25 15:00:42.652087: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "data_augmentation = tf.keras.Sequential(\n",
    "    [\n",
    "        layers.Normalization(),\n",
    "        layers.Resizing(image_size, image_size),\n",
    "        #layers.RandomFlip(\"horizontal\"),\n",
    "        #layers.RandomRotation(factor=0.02),\n",
    "        #layers.RandomZoom(\n",
    "        #    height_factor=0.2, width_factor=0.2\n",
    "        #),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d6a2030",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfe168e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8eda51d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super().__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim) #Linear Projection Of Patch-Tockens\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "    \n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27373260",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Double_Q_network(s,a,Neurons):\n",
    "    state= Input(shape=(s,))\n",
    "    action=Input(shape=(a,))\n",
    "    \n",
    "    x_1 = Dense(Neurons, activation=\"relu\")(state)\n",
    "    x_1 = Dense(40, activation=\"relu\")(x_1)\n",
    "   \n",
    "    x_2 = Dense(Neurons, activation=\"relu\")(action)\n",
    "    x_2 = Dense(40, activation=\"relu\")(x_2)\n",
    "    \n",
    "    x=concatenate([x_1,x_2],axis=-1)\n",
    "    \n",
    "    theta_Q=Dense(20, activation=\"relu\")(x)\n",
    "    theta_Q=Dense(1, activation=\"relu\")(theta_Q)\n",
    "    \n",
    "    theta_net =Activation(\"linear\", name=\"theta_output\")(theta_Q)\n",
    "    \n",
    "    velocity_Q=Dense(20, activation=\"relu\")(x)\n",
    "    velocity_Q=Dense(1, activation=\"relu\")(velocity_Q)\n",
    "    \n",
    "    velocity_net =Activation(\"linear\", name=\"vel_output\")(velocity_Q)\n",
    "    \n",
    "    losses={'theta_output':'mse','vel_output':'mse'}\n",
    "    loss_weight = {\"theta_output\": 1.0, \"vel_output\": 1.0}\n",
    "    \n",
    "    Q_1 = Model(inputs = [state,action],outputs = [theta_net,velocity_net],name='first_Q')\n",
    "    \n",
    "    Q_1.compile(optimizer=Adam(learning_rate = alpha), loss=losses,loss_weights = loss_weight)# a dict must be passed to loss\n",
    "    \n",
    "    y_1 = Dense(Neurons, activation=\"relu\")(state)\n",
    "    y_1 = Dense(40, activation=\"relu\")(y_1)\n",
    "   \n",
    "    y_2 = Dense(Neurons, activation=\"relu\")(action)\n",
    "    y_2 = Dense(40, activation=\"relu\")(y_2)\n",
    "\n",
    "    y=concatenate([y_1,y_2],axis=-1)\n",
    "    \n",
    "    theta_Q_2=Dense(20, activation=\"relu\")(y)\n",
    "    theta_Q_2=Dense(1, activation=\"relu\")(theta_Q_2)\n",
    "    \n",
    "    theta_net_2 =Activation(\"linear\", name=\"theta2_output\")(theta_Q_2)\n",
    "    \n",
    "    velocity_Q_2=Dense(20, activation=\"relu\")(y)\n",
    "    velocity_Q_2=Dense(1, activation=\"relu\")(velocity_Q_2)\n",
    "    \n",
    "    velocity_net_2 =Activation(\"linear\", name=\"vel2_output\")(velocity_Q_2)\n",
    "    \n",
    "    losses_2={'theta2_output':'mse','vel2_output':'mse'}\n",
    "    loss_weight_2 = {\"theta2_output\": 1.0, \"vel2_output\": 1.0}\n",
    "    \n",
    "    Q_2 = Model(inputs = [state,action],outputs = [theta_net_2,velocity_net_2],name='second_Q')\n",
    "    \n",
    "    Q_2.compile(optimizer=Adam(learning_rate = alpha), loss=losses_2,loss_weights = loss_weight_2)\n",
    "    \n",
    "    return Q_1,Q_2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e78c55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_ext():\n",
    "    image_inputs = Input(shape=input_shape) #shape=(None, 32, 32, 3)\n",
    "    goal_inputs = Input(shape=(2,))\n",
    "    # Augment data.\n",
    "    augmented = data_augmentation(image_inputs) #shape=(None, 72, 72, 3)\n",
    "    # Create patches.\n",
    "    patches = Patches(patch_size)(augmented) #shape=(None, None, 108)\n",
    "    \n",
    "    #Add Goal to Patches\n",
    "    Goal = mlp(goal_inputs,[32,512,3072],dropout_rate=0.05) #shape=(1, 192)\n",
    "    Goal = tf.reshape(Goal,[1,1,3072])\n",
    "\n",
    "    tf.keras.layers.Concatenate(axis=1)([Goal,patches])    \n",
    "    # Encode patches.\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches) #shape=(None, 144, 64)\n",
    "    \n",
    "    \n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention( #Self Attention mechanism\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "        # Skip connection 2.\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Create a [batch_size, projection_dim] tensor.\n",
    "        representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        representation = layers.Flatten()(representation)\n",
    "        representation = layers.Dropout(0.3)(representation)\n",
    "        #so far we created the image representation\n",
    "    \n",
    "        # Add MLP.\n",
    "        features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
    "        \n",
    "    model = Model(inputs=[image_inputs,goal_inputs], outputs=[features])             \n",
    "    return model\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "300a5e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Actor(state_Dims):\n",
    "    features = layers.Input(shape=(1024,))\n",
    "    \n",
    "    linear_vel = layers.Dense(512, activation=\"relu\" , name = 'first_lin')(features)\n",
    "    linear_vel = layers.Dense(128, activation=\"relu\")(linear_vel)\n",
    "    linear_vel = layers.Dense(32, activation=\"relu\")(linear_vel)\n",
    "    linear_vel = layers.Dense(1,)(linear_vel)                        \n",
    "    linear_net =Activation(\"linear\", name=\"linear_output\")(linear_vel)                  \n",
    "     \n",
    "     \n",
    "                                                                                         \n",
    "    angular_vel = layers.Dense(512, activation=\"relu\",name = 'first_ang')(features)\n",
    "    angular_vel = layers.Dense(128, activation=\"relu\")(angular_vel)\n",
    "    angular_vel = layers.Dense(32, activation=\"relu\")(angular_vel)\n",
    "    angular_vel = layers.Dense(1,)(linear_vel)                        \n",
    "    angular_net =Activation(\"linear\", name=\"angular_output\")(angular_vel)\n",
    "    \n",
    "    model = Model(inputs = features , outputs = [linear_vel,angular_vel])\n",
    "    \n",
    "    return model\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f1cf296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_net(state_dim,Neurons):\n",
    "    indim_1=Input(shape=(state_dim,))\n",
    "    \n",
    "    x=Dense(Neurons,activation=\"relu\")(indim_1)\n",
    "    x=Dense(30,activation=\"relu\")(x)\n",
    "    x=Dense(1,activation=\"relu\")(x)\n",
    "    \n",
    "    value=Activation(\"linear\",name=\"val_out\")(x)\n",
    "    \n",
    "    value_net=Model(inputs=indim_1 , outputs=value , name=\"val_net\")\n",
    "    \n",
    "    value_net.compile(optimizer=Adam(learning_rate = alpha), loss='mse')\n",
    "    \n",
    "    return value_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b02e092a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_value_net(state_dim,Neurons):\n",
    "    indim_2=Input(shape=(state_dim,))\n",
    "    \n",
    "    x=Dense(Neurons,activation=\"relu\")(indim_2)\n",
    "    x=Dense(30,activation=\"relu\")(x)\n",
    "    x=Dense(1,activation=\"relu\")(x)\n",
    "    \n",
    "    value=Activation(\"linear\",name=\"val_out\")(x)\n",
    "    \n",
    "    target_value_net=Model(inputs=indim_2 , outputs=value , name=\"t_val_net\")\n",
    "   \n",
    "    \n",
    "    return target_value_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "909fe6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainable_reward_func(in_Dims):\n",
    "    features= Input(shape=(in_Dims,))\n",
    "    \n",
    "    linear_vel = Dense(512, activation=\"relu\" , name = 'first_lin')(features)\n",
    "    linear_vel = Dense(128, activation=\"relu\")(linear_vel)\n",
    "    linear_vel = Dense(32, activation=\"relu\")(linear_vel)\n",
    "    linear_vel = Dense(1,)(linear_vel)                        \n",
    "    linear_net =Activation(\"linear\", name=\"linear_output\")(linear_vel)                  \n",
    "     \n",
    "     \n",
    "                                                                                         \n",
    "    angular_vel = Dense(512, activation=\"relu\",name = 'first_ang')(features)\n",
    "    angular_vel = Dense(128, activation=\"relu\")(angular_vel)\n",
    "    angular_vel = Dense(32, activation=\"relu\")(angular_vel)\n",
    "    angular_vel = Dense(1,)(linear_vel)                        \n",
    "    angular_net =Activation(\"linear\", name=\"angular_output\")(angular_vel)\n",
    "    \n",
    "    ConCat = concatenate([linear_net , angular_net] , axis = -1)\n",
    "    \n",
    "    x=Dense(32,activation=\"relu\")(ConCat)\n",
    "    x=Dense(16,activation=\"relu\")(x)\n",
    "    x=Dense(1)(x)\n",
    "    \n",
    "    out=Activation(\"linear\",name=\"rew_out\")(x)\n",
    "    \n",
    "    \n",
    "    reward_net = Model(inputs=features,outputs=out,name=\"TPr_net\")\n",
    "    \n",
    "    return reward_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a31aeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftActorCritic:\n",
    "\n",
    "    def __init__(self, action_dim,state_dim, epoch_step=1, learning_rate=0.0003,alpha=0.2, gamma=0.99):\n",
    "        self.Actor = Actor(state_dim)\n",
    "        self.q_net = Double_Q_network(state_dim,action_dim,30)  \n",
    "        self.v_net = value_net(state_dim,10)\n",
    "        self.target_v_net = target_value_net(state_dim,10)\n",
    "        self.backBone = feature_ext()\n",
    "        self.reward_net = trainable_reward_func(state_dim) \n",
    "        self.log_dir = '~/project/model'\n",
    "        \n",
    "        self.backBone.load_weights(self.log_dir + 'Back_bone.h5')\n",
    "        self.Actor.load_weights(self.log_dir + 'actor.h5')\n",
    "        self.reward_net.load_weights(self.log_dir + 'reward.h5') \n",
    "        #self.reward=5.0\n",
    "        \n",
    "        self.memory = memory=deque([], maxlen=2500)\n",
    "        self.epoch = epoch_step\n",
    "        \n",
    "        \n",
    "        self.actor_optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "        #self.reward_optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "\n",
    "\n",
    "    def sample_action(self,state,goal):\n",
    "        st = self.backBone([state,goal])\n",
    "        th,vel=self.Actor(st)\n",
    "        return th,vel\n",
    "    \n",
    "    def random_goal(self):\n",
    "        return np.random.rand(2)\n",
    "    \n",
    "    def sample_state(self):\n",
    "        return np.random.rand(160,120,3)\n",
    "\n",
    "    def update_D_Qn(self,n_state,action):\n",
    "        n_st_value = self.target_v_net(n_state)\n",
    "        target_Q = self.reward_net(n_state) + n_st_value\n",
    "        \n",
    "        \n",
    "        self.q_net[0].fit(x=[n_state,action], y={'theta_output':target_Q,'vel_output':target_Q}, epochs = self.epoch)\n",
    "        self.q_net[1].fit(x=[n_state,action], y={'theta2_output':target_Q,'vel2_output':target_Q}, epochs = self.epoch)\n",
    "        \n",
    "    def update_value_net(self,st,act):\n",
    "            first_Q_Double=tf.stop_gradient(self.q_net[0].predict([st,act]))\n",
    "            second_Q_Double=tf.stop_gradient(self.q_net[1].predict([st,act]))\n",
    "\n",
    "            Q_th_f=first_Q_Double[0][:]\n",
    "            Q_vel_f=first_Q_Double[1][:]\n",
    "\n",
    "\n",
    "            Q_th_sec=second_Q_Double[0][:]\n",
    "            Q_vel_sec=second_Q_Double[1][:]\n",
    "\n",
    "\n",
    "            Q_th=tf.minimum(Q_th_f,Q_th_sec)\n",
    "            Q_vel=tf.minimum(Q_vel_f,Q_th_sec)\n",
    "            \n",
    "            act_th = act[0:batch_size,0].reshape(batch_size,1)\n",
    "            act_vel = act[0:batch_size,1].reshape(batch_size,1)\n",
    "            \n",
    "            act_th = abs(act_th) + 0.0001\n",
    "            act_vel = abs(act_vel) + 0.0001\n",
    "            \n",
    "            bel_back_th = Q_th - np.log10(act_th)\n",
    "            bel_back_vel = Q_vel - np.log10(act_vel)\n",
    "            \n",
    "            res = tf.add(Q_th,Q_vel)\n",
    "            bel_back = tf.multiply(tf.constant([0.5]) , res)\n",
    "\n",
    "            self.v_net.fit(st, bel_back,epochs=1, verbose=0)\n",
    "\n",
    "    def updateTargetModel(self,tau=0.2):\n",
    "        weights= self.v_net.get_weights()\n",
    "        target_net = self.target_v_net.get_weights()\n",
    "        \n",
    "        for i in range(len(weights)):\n",
    "\n",
    "            new_weight = weights[i]+ (1-tau)*target_net[i]\n",
    "            target_net[i] = new_weight\n",
    " \n",
    "        self.target_v_net.set_weights(target_net)\n",
    " \n",
    "        return self.target_v_net    \n",
    "    \n",
    "        \n",
    "    def memory_store(self,state, action, next_state, goal,proccessed_state):\n",
    "    \n",
    "        self.memory.append((state, action, next_state, goal,proccessed_state))\n",
    "        return self.memory\n",
    "      \n",
    "    def train_rewardNet(self,st):\n",
    "        \n",
    "        with tf.GradientTape() as grad:\n",
    "            \n",
    "            \n",
    "            \n",
    "            reward = self.reward_net(st)\n",
    "          \n",
    "            \n",
    "            act = tf.stop_gradient(self.Actor(st))\n",
    "            act=np.array(act).reshape(10,2)\n",
    "            \n",
    "            first_Q_Double = tf.stop_gradient(self.q_net[0]([st,act]))\n",
    "            second_Q_Double = tf.stop_gradient(self.q_net[1]([st,act]))\n",
    "            act_th , act_vel = act[:,0].reshape(10,1),act[:,1].reshape(10,1)\n",
    "           \n",
    "            \n",
    "            Q_th_f=first_Q_Double[0][:]\n",
    "            Q_vel_f=first_Q_Double[1][:]\n",
    "\n",
    "\n",
    "            Q_th_sec=second_Q_Double[0][:]\n",
    "            Q_vel_sec=second_Q_Double[1][:]\n",
    "\n",
    "\n",
    "            Q_th=tf.minimum(Q_th_f,Q_th_sec)\n",
    "            Q_vel=tf.minimum(Q_vel_f,Q_th_sec)\n",
    "         \n",
    "            loss = -tf.reduce_mean(tf.multiply(tf.constant([0.5]), reward + Q_th + Q_vel - tf.math.log(act_vel+0.0001)- tf.math.log(act_th+0.0001)))\n",
    "          \n",
    "            \n",
    "        model_gradients = grad.gradient(loss , self.reward_net.trainable_weights)\n",
    "        self.reward_optimizer.apply_gradients(zip(model_gradients , self.reward_net.trainable_weights))\n",
    "        \n",
    "            \n",
    "        \n",
    "    def train_Actor(self,state,goal):\n",
    "        \n",
    "        with tf.GradientTape() as grad:\n",
    "            st = self.backBone([state,goal])\n",
    "            act = self.Actor(st)\n",
    "            reward = self.reward_net(st)\n",
    "            \n",
    "            act_th , act_vel= self.Actor(st)\n",
    "            act=np.array(act).reshape(10,2)\n",
    "            \n",
    "            \n",
    "            \n",
    "            first_Q_Double=tf.stop_gradient(self.q_net[0]([st,act]))\n",
    "            second_Q_Double=tf.stop_gradient(self.q_net[1]([st,act]))\n",
    "            \n",
    "            Q_th_f=first_Q_Double[0][:]\n",
    "            Q_vel_f=first_Q_Double[1][:]\n",
    "\n",
    "\n",
    "            Q_th_sec=second_Q_Double[0][:]\n",
    "            Q_vel_sec=second_Q_Double[1][:]\n",
    "\n",
    "\n",
    "            Q_th=tf.minimum(Q_th_f,Q_th_sec)\n",
    "            Q_vel=tf.minimum(Q_vel_f,Q_th_sec)\n",
    "            \n",
    "\n",
    "            action_loss= -tf.reduce_mean(tf.multiply(tf.constant([0.5]),Q_th - tf.math.log(act_th+0.0001) + Q_vel - tf.math.log(act_vel+0.0001)))\n",
    "            reward_loss = -tf.reduce_mean(tf.multiply(tf.constant([0.5]), reward + Q_th + Q_vel - tf.math.log(act_vel+0.0001)- tf.math.log(act_th+0.0001)))            \n",
    "        \n",
    "\n",
    "            \n",
    "        model_gradients = grad.gradient([action_loss , reward_loss],self.Actor.trainable_weights)\n",
    "        self.actor_optimizer.apply_gradients(zip(model_gradients, self.Actor.trainable_variables))\n",
    "      \n",
    "        return \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a12c0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(memory , batch_size=10):\n",
    "    \n",
    "    minibatch = random.sample(memory,batch_size)\n",
    "    minibatch = np.array(minibatch,dtype=object)\n",
    "    st=[]\n",
    "    act=[]\n",
    "    n_st=[]\n",
    "    goal = []\n",
    "    pro_state= []\n",
    "    for i in range(len(minibatch)):\n",
    "        st.append(minibatch[i][0])\n",
    "        act.append(minibatch[i][1])\n",
    "        n_st.append(minibatch[i][2])\n",
    "        goal.append(minibatch[i][3])\n",
    "        pro_state.append(minibatch[i][4][0])\n",
    "        \n",
    "    st=np.array(st)\n",
    "    n_st=np.array(n_st)\n",
    "    act=np.array(act).reshape(batch_size,2)\n",
    "    goal=np.array(goal)\n",
    "    pro_state=np.array(pro_state)\n",
    "\n",
    "    sac.update_value_net(pro_state,act)\n",
    "    sac.update_D_Qn(pro_state,act)\n",
    "    sac.train_Actor(st,goal)\n",
    "    sac.updateTargetModel()\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ff091c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Unable to open file (unable to open file: name = 'D:\\Ali_Reza\\kir to hossein\\model\\Back_bone.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# initializing the network_models\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m sac \u001b[38;5;241m=\u001b[39m \u001b[43mSoftActorCritic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(M):\n\u001b[1;32m      7\u001b[0m         Return \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "Cell \u001b[0;32mIn[14], line 12\u001b[0m, in \u001b[0;36mSoftActorCritic.__init__\u001b[0;34m(self, action_dim, state_dim, epoch_step, learning_rate, alpha, gamma)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward_net \u001b[38;5;241m=\u001b[39m trainable_reward_func(state_dim) \n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mAli_Reza\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mkir to hossein\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackBone\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBack_bone.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mActor\u001b[38;5;241m.\u001b[39mload_weights(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_dir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactor.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward_net\u001b[38;5;241m.\u001b[39mload_weights(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_dir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreward.h5\u001b[39m\u001b[38;5;124m'\u001b[39m) \n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/h5py/_hl/files.py:567\u001b[0m, in \u001b[0;36mFile.__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    558\u001b[0m     fapl \u001b[38;5;241m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[1;32m    559\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[1;32m    560\u001b[0m                      alignment_threshold\u001b[38;5;241m=\u001b[39malignment_threshold,\n\u001b[1;32m    561\u001b[0m                      alignment_interval\u001b[38;5;241m=\u001b[39malignment_interval,\n\u001b[1;32m    562\u001b[0m                      meta_block_size\u001b[38;5;241m=\u001b[39mmeta_block_size,\n\u001b[1;32m    563\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    564\u001b[0m     fcpl \u001b[38;5;241m=\u001b[39m make_fcpl(track_order\u001b[38;5;241m=\u001b[39mtrack_order, fs_strategy\u001b[38;5;241m=\u001b[39mfs_strategy,\n\u001b[1;32m    565\u001b[0m                      fs_persist\u001b[38;5;241m=\u001b[39mfs_persist, fs_threshold\u001b[38;5;241m=\u001b[39mfs_threshold,\n\u001b[1;32m    566\u001b[0m                      fs_page_size\u001b[38;5;241m=\u001b[39mfs_page_size)\n\u001b[0;32m--> 567\u001b[0m     fid \u001b[38;5;241m=\u001b[39m make_fid(name, mode, userblock_size, fapl, fcpl, swmr\u001b[38;5;241m=\u001b[39mswmr)\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libver \u001b[38;5;241m=\u001b[39m libver\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/h5py/_hl/files.py:231\u001b[0m, in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n\u001b[1;32m    230\u001b[0m         flags \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mACC_SWMR_READ\n\u001b[0;32m--> 231\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    233\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, h5f\u001b[38;5;241m.\u001b[39mACC_RDWR, fapl\u001b[38;5;241m=\u001b[39mfapl)\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5f.pyx:106\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to open file (unable to open file: name = 'D:\\Ali_Reza\\kir to hossein\\model\\Back_bone.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# initializing the network_models\n",
    "sac = SoftActorCritic(2,1024)\n",
    "\n",
    "\n",
    "\n",
    "for episode in range(M):\n",
    "        Return = 0\n",
    "        # Here you should define youre environment\n",
    "        # Defining the initial state \n",
    "        # including reset function\n",
    "        \n",
    "        \n",
    "        for t in range(T):\n",
    "            \n",
    "            state= sac.sample_state() # sample state take the states from actual scene : this part must be comented\n",
    "            goal = sac.random_goal()  # fake goal we need a real one\n",
    "            \n",
    "            state_2 = np.expand_dims(state,axis = 0)\n",
    "            goal_2 = np.expand_dims(goal,axis=0)\n",
    "            \n",
    "            proccessed_state = sac.backBone([state_2,goal_2])\n",
    "            reward = sac.reward_net(proccessed_state)\n",
    "            \n",
    "            Return = Return + reward\n",
    "            \n",
    "            action= sac.sample_action(state_2,goal_2) # take the action and goes to the naxt state : observe the next state from the env\n",
    "            n_state= sac.sample_state()  # it should be the actual state\n",
    "            sac.memory_store(state,action,n_state,goal,proccessed_state) \n",
    "            \n",
    "\n",
    "            \n",
    "            if len(sac.memory)>batch_size:\n",
    "                memory = sac.memory\n",
    "                train(memory)\n",
    "            state=n_state\n",
    "    \n",
    "        print(f' #################[info] ... the Return of the trajectory {episode + 1} equals to {Return} #################')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1264a6d5",
   "metadata": {},
   "source": [
    "# the only left thing is to get the reward and return the Return of the trajectory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe0492e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc18d91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5336c06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ef2c88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4b8fc8fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6913b031",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f9411b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9168bd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a71dec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
